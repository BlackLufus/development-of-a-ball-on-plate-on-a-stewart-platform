# Development of a ball-on-plate prototype on a Stewart platform with comparison between PID and reinforcement learning

## ðŸ“Œ Project description

This project deals with the **development of a ball-on-plate prototype** on a **Stewart platform**.  
The goal is to stabilize and control the ball precisely on a moving plateâ€”both with a **classical PID controller** and with a **reinforcement learning agent (RL)**.  
By comparing these two approaches, the strengths and weaknesses of model-based and learning-based control systems will be investigated.

## ðŸ› ï¸ General conditions

The implementation of this project is based entirely on existing components:

- Stewart platform with 6 actuators
- Six EMAX-ES09MD servo motors
- Logitech Brio for ball position determination
- Nvidia Jetson TX2 Development Kit for controlling servo motors via the I2C interface

### ðŸ’» System requirements

- **Betriebssystem** Ubuntu 18.04
- **Python** 3.12.10  
- **Node.js** v20.17.0
- **NPM Version** 10.8.2
- **tsc Version** 5.8.3

### ðŸ“¦ Required Python packages

The required dependencies can be installed via `requirements.txt`:

```bash
pip install -r requirements.txt
```

## ðŸª„ Compiling TypeScript to JavaScript

The web front end of the project is based on **TypeScript** and must be converted to **JavaScript** before execution.

### 1. Install dependencies
Navigate to the `web/frontend` directory and install the required npm packages:

```bash
cd src/web/frontend
npm install
```

### 2. Compiling TypeScript

Compilation is performed using the tsconfig.json file included in the project.
To do this, execute the following command in the main directory or in the frontend folder:

```bash
npx tsc
```

## ðŸ“‚ Project structure

```bash
ball-on-plate/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ bop
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ 4_0
â”œâ”€â”€ node_modules/
â”œâ”€â”€ recorded_images/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ball_on_plate/
â”‚   â”‚   â”œâ”€â”€ pid/
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ v3/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ physical/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ agent.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ simulation/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ agent.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ pid_simulate_ball_on_plate_v3.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ task.py
â”‚   â”‚   â”œâ”€â”€ rl/
â”‚   â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ v4/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ physical/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ environment.py
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ training.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ simulation/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ environment.py
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ training.py
â”‚   â”‚   â”‚   â””â”€â”€ task.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ detection/
â”‚   â”‚   â”œâ”€â”€ opencv/
â”‚   â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”‚   â”œâ”€â”€ ball_tracker copy.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ball_tracker_approach_1.ipynb
â”‚   â”‚   â”‚   â”œâ”€â”€ ball_tracker_approach_2.ipynb
â”‚   â”‚   â”‚   â””â”€â”€ ball_tracker.py
â”‚   â”‚   â””â”€â”€ yolo/
â”‚   â”‚       â”œâ”€â”€ ac_detection.py
â”‚   â”‚       â”œâ”€â”€ cv_detection.py
â”‚   â”‚       â””â”€â”€ train_yolo_with_ball_on_plate.ipynb
â”‚   â”œâ”€â”€ nunchuck/
â”‚   â”‚   â”œâ”€â”€ nunchuck.py
â”‚   â”‚   â””â”€â”€ task.py
â”‚   â”œâ”€â”€ stewart_plattform/
â”‚   â”‚   â”œâ”€â”€ PCA9685/
â”‚   â”‚   â”‚   â”œâ”€â”€ PCA9685.py
â”‚   â”‚   â”œâ”€â”€ servo_motor_handler.py
â”‚   â”‚   â”œâ”€â”€ slider.py
â”‚   â”‚   â”œâ”€â”€ stewart_plattform.py
â”‚   â”‚   â””â”€â”€ task.py
â”‚   â”œâ”€â”€ vacuum_robot/
â”‚   â”‚   â”œâ”€â”€ analyse/
â”‚   â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ v0/
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ v3/
â”‚   â”‚   â”‚   â”œâ”€â”€ agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ environment.py
â”‚   â”‚   â”‚   â””â”€â”€ training.py
â”‚   â”‚   â””â”€â”€ q_table.py
â”‚   â”œâ”€â”€ video_capture/
â”‚   â”‚   â”œâ”€â”€ video_capture.py
â”‚   â”‚   â””â”€â”€ task.py
â”‚   â”œâ”€â”€ web/
â”‚   â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”‚   â”œâ”€â”€ consumer.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ frontend/
â”‚   â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â”œâ”€â”€ styles/
â”‚   â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”‚   â”œâ”€â”€ server/
â”‚   â”‚   â”‚   â”œâ”€â”€ asgi.py
â”‚   â”‚   â”‚   â”œâ”€â”€ settings.py
â”‚   â”‚   â”‚   â”œâ”€â”€ urls.py
â”‚   â”‚   â”‚   â””â”€â”€ wsgi.py
â”‚   â”‚   â””â”€â”€ manager.py
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ parser_manager.py
â”‚   â”œâ”€â”€ tensorboard/
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ bop
â”‚   â”‚       â”œâ”€â”€ ...
â”‚   â”‚       â””â”€â”€ 4_0
â”‚   â””â”€â”€ venv/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

## ðŸ§© Additional modules

```nunchuck/``` â€“ Nunchuck control for manual platform control

```video_capture/``` â€“ Direct camera access

```stewart_platform/``` â€“ Low-level servo control, kinematics, PCA9685 control

```parser_manager.py``` â€“ Configuration management via **config.json**

## ðŸŽ¥ Checking Connected Cameras

Before using the recorder, you can verify which camera is connected and what settings are available.

### 1. List all connected video devices

```bash
v4l2-ctl --list-devices
```

This command shows all cameras detected by your system along with their device paths (e.g., `/dev/video0`, `/dev/video1`).

### 2. Check supported formats and options for a specific device

```bash
v4l2-ctl --device=/dev/video1 --list-formats-ext
```

Replace `/dev/video1` with the device path of your camera. This will display all supported resolutions, frame rates, and pixel formats.

> âš ï¸ Note: `v4l2-ctl`is typically available on Linux systems. Windows users will need to use their systemâ€™s camera settings or other tools.

## ðŸ“Š Viewing Logs with TensorBoard

You can monitor training progress, metrics, and visualizations using TensorBoard.

### 1. Start TensorBoard

```bash
tensorboard --logdir ./tensorboard/
```

This command starts a local TensorBoard server using the logs stored in `./tensorboard/`.

### 2. Open in a browser

After starting, TensorBoard will provide a local URL (e.g., `http://localhost:6006`). Open this in your browser to explore your training metrics, graphs, and summaries.

> âš ï¸ Make sure TensorBoard is installed in your Python environment. You can install it with:
> ```bash
> pip install tensorboard
> ```

## ðŸš€ Application

Some functions can be executed directly via main:

```bash
python -m src.main --help
```

### Control the Stewart platform directly

```bash
python -m src.main --run set
```

or

```bash
python -m src.main --run circle
```

### Nunchuck control

```bash
python -m src.main --run nunchuck
```

### Video capture

```bash
python -m src.main --run video_capture_linux
```

### Run simulation (RL or PID)

```bash
python -m src.main --run ball_on_plate
```

### Execute/train simulation (RL or PID)

```bash
python -m src.ball_on_plate.rl.v4.simulation.train
```

or

```bash
python -m src.ball_on_plate.pid.v3.simulation.agent
```

### Perform/train physically (RL or PID)

```bash
python -m src.ball_on_plate.rl.v4.physical.train
```

or

```bash
python -m src.ball_on_plate.pid.v3.physical.agent
```

### Image recorder

```bash
python -m src.main --run image_recorder -f 15 -t 200 --os 'linux'
```

### Web interface

```bash
python -m src.web.manage runserver 127.0.0.1:8000
```

## Download models

> [rl_model_4_0](https://drive.google.com/file/d/1qXxIWzX0APrrYirxvvCHgpOYDEqDvQCG/view?usp=drive_link)

## ðŸ§¾ Lizenz

This project is licensed under the `MIT` License.\\
For details, see LICENSE.
